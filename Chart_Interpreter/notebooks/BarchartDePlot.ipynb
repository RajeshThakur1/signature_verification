{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb6ff2-7e2e-46b2-8cb8-64d0c1dfa095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib pandas\n",
    "!pip install gradio openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac8a68-c0aa-4246-9d05-18c7ea45eeae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.8/site-packages (2.14.5)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.33.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.23.4)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.17.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157c150-208f-4bc3-9d5f-9cce7f9f87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a7132-0247-43e9-8493-b933db0b4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a9991-5a83-4115-92b3-fd30065f1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_names = [\n",
    "#     'Red', 'Green', 'Blue', 'Orange', 'Purple',\n",
    "#     'Yellow', 'Cyan', 'Magenta', 'Lime', 'Pink',\n",
    "#     'Teal', 'Indigo', 'Turquoise', 'Brown', 'Gray',\n",
    "#     'Maroon', 'Olive', 'Navy', 'Silver',\n",
    "#     'White', 'Gold', 'Violet', 'Crimson',\n",
    "#     'Plum', 'Lavender', 'Salmon']\n",
    "\n",
    "# def get_random_color():\n",
    "#     return random.choice(color_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8299a92-5c5f-49e6-8a84-4329f5c1cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_image_caption_dataset(photos_gen):\n",
    "#     dataset = []\n",
    "\n",
    "#     # Create a directory to store the PNG files and JSON data\n",
    "#     data_folder = 'Data_Barcharts'\n",
    "#     os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "#     # Generate bar chart images and corresponding text data\n",
    "#     for i in range(photos_gen):\n",
    "#         data_values = [random.randint(1, random.randint(1, 3000)) for _ in range(5)]\n",
    "\n",
    "#         # Create text description for the bar chart\n",
    "#         chart_text = f\"\"\"Title:Bar Chart {i + 1}, x-axis: Category, y-axis: Values   \"\"\"\n",
    "\n",
    "#         colors = [get_random_color() for _ in range(5)]\n",
    "#         color_info = \", \".join([f\"{category}: Value={value}\" for category, value, color in zip(['A', 'B', 'C', 'D', 'E'], data_values, colors)])\n",
    "#         chart_text += f\"{color_info}\"\n",
    "\n",
    "#         data = {\n",
    "#             #'image_path': os.path.join(data_folder, f'bar_chart_{i + 1}.png'),\n",
    "#             'image':f\"chart_{i+1}.png\",\n",
    "#             'text': chart_text\n",
    "#         }\n",
    "\n",
    "#         # Create a figure for the bar chart\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#         ax.bar(['A', 'B', 'C', 'D', 'E'], data_values, color=colors)\n",
    "#         ax.set_xlabel('Category')\n",
    "#         ax.set_ylabel('Values')\n",
    "#         ax.set_title(f'Bar Chart {i + 1}')\n",
    "\n",
    "#         # Save the figure with a transparent background to the Google Drive folder\n",
    "#         fig.patch.set_facecolor('none')\n",
    "#         image_filename = os.path.join(data_folder, f'chart_{i + 1}.png')\n",
    "#         fig.savefig(image_filename, transparent=True)\n",
    "\n",
    "#         # Close the figure to free up memory\n",
    "#         plt.close(fig)\n",
    "\n",
    "#         # Append the data to the dataset\n",
    "#         dataset.append(data)\n",
    "\n",
    "#     # Save the dataset as a JSON file\n",
    "#     json_filename = os.path.join(data_folder, 'BarChart_DePlot.json')\n",
    "#     with open(json_filename, 'w') as json_file:\n",
    "#         json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "# # Generate the dataset\n",
    "# generate_image_caption_dataset(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de7b94-1073-4f67-a93c-4a6e8bfbc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Data_Barcharts/BarChart_DePlot.json', 'r') as file:\n",
    "#     # Read the entire file contents into a string\n",
    "#     file_contents = file.read()\n",
    "\n",
    "# json_data = json.loads(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a3384-c1e2-419e-9105-f2e71114a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369b6d9725d344c1b7ae39c15cb70880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "# dataset = []\n",
    "# def generator():\n",
    "#   for number in range(50):\n",
    "#     json_object = json_data[number]\n",
    "#     #txt = \"\"\n",
    "#     # for category in json_object[\"Categories\"]:\n",
    "#     #   title = category[\"Category\"]\n",
    "#     #   value = category[\"Value\"]\n",
    "#     #   color = category[\"Color\"]\n",
    "#     #   txt += f\"{title}: {value}, color:{color}\\n\"\n",
    "#     image=Image.open(f\"Data_Barcharts/chart_{number+1}.png\")\n",
    "#     yield {\"image\":image,\"text\":json_object[\"text\"]}\n",
    "# dataset=Dataset.from_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aadcf4-34ee-4ebc-9c82-441b0ca87b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a5306-7e25-4c19-9891-547b59e4d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# MAX_PATCHES = 1024\n",
    "\n",
    "# class ImageCaptioningDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, dataset, processor):\n",
    "#         self.dataset = dataset\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.dataset[idx]\n",
    "#         encoding = self.processor(images=item[\"image\"],text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n",
    "\n",
    "#         encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "#         encoding[\"text\"] = item[\"text\"]\n",
    "#         return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ed985-ce85-4175-a3df-3fa0dff7b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/deplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b5976-3271-46ac-b9d1-87bb3ab6f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collator(batch):\n",
    "#   processor.image_processor.is_vqa = False\n",
    "#   new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "#   texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "#   text_inputs = processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=500)\n",
    "\n",
    "#   new_batch[\"labels\"] = text_inputs.input_ids\n",
    "\n",
    "#   for item in batch:\n",
    "#     new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "#     new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "\n",
    "#   new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "#   new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "#   return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa3cd8-6ac9-4b25-84bb-8635de5a3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780fb2d-1384-4582-8541-03f90334c1e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.9716769456863403\n",
      "Loss: 1.97222900390625\n",
      "Loss: 1.9725173711776733\n",
      "Loss: 1.9722219705581665\n",
      "Loss: 1.978918194770813\n",
      "Loss: 1.9738649129867554\n",
      "Loss: 1.9850165843963623\n",
      "Loss: 1.9632948637008667\n",
      "Loss: 1.971656084060669\n",
      "Loss: 1.9586364030838013\n",
      "Loss: 1.972122073173523\n",
      "Loss: 1.9519139528274536\n",
      "Loss: 1.9715677499771118\n",
      "Loss: 1.974493384361267\n",
      "Loss: 1.9640823602676392\n",
      "Loss: 1.9686940908432007\n",
      "Loss: 1.9445862770080566\n",
      "Loss: 1.9651696681976318\n",
      "Loss: 1.9529516696929932\n",
      "Loss: 1.9458104372024536\n",
      "Loss: 1.947068452835083\n",
      "Loss: 1.9465943574905396\n",
      "Loss: 1.9460797309875488\n",
      "Loss: 1.9533276557922363\n",
      "Loss: 1.9428504705429077\n",
      "Epoch: 1\n",
      "Loss: 1.9352664947509766\n",
      "Loss: 1.9380359649658203\n",
      "Loss: 1.9358035326004028\n",
      "Loss: 1.9184156656265259\n",
      "Loss: 1.920832633972168\n",
      "Loss: 1.9181630611419678\n",
      "Loss: 1.9221458435058594\n",
      "Loss: 1.929468035697937\n",
      "Loss: 1.9278011322021484\n",
      "Loss: 1.927354335784912\n",
      "Loss: 1.9333925247192383\n",
      "Loss: 1.9173585176467896\n",
      "Loss: 1.9166229963302612\n",
      "Loss: 1.905574083328247\n",
      "Loss: 1.912713885307312\n",
      "Loss: 1.9069629907608032\n",
      "Loss: 1.923749566078186\n",
      "Loss: 1.907417893409729\n",
      "Loss: 1.9121918678283691\n",
      "Loss: 1.9122354984283447\n",
      "Loss: 1.907623529434204\n",
      "Loss: 1.8952757120132446\n",
      "Loss: 1.909566044807434\n",
      "Loss: 1.894989252090454\n",
      "Loss: 1.9150606393814087\n",
      "Epoch: 2\n",
      "Loss: 1.8984395265579224\n",
      "Loss: 1.8875527381896973\n",
      "Loss: 1.890299916267395\n",
      "Loss: 1.8758820295333862\n",
      "Loss: 1.8844752311706543\n",
      "Loss: 1.878562331199646\n",
      "Loss: 1.8791682720184326\n",
      "Loss: 1.8756839036941528\n",
      "Loss: 1.8813629150390625\n",
      "Loss: 1.8749806880950928\n",
      "Loss: 1.8744916915893555\n",
      "Loss: 1.8830980062484741\n",
      "Loss: 1.8713575601577759\n",
      "Loss: 1.8799692392349243\n",
      "Loss: 1.8674581050872803\n",
      "Loss: 1.852004051208496\n",
      "Loss: 1.8628381490707397\n",
      "Loss: 1.8611980676651\n",
      "Loss: 1.8732130527496338\n",
      "Loss: 1.8546849489212036\n",
      "Loss: 1.8490233421325684\n",
      "Loss: 1.8435032367706299\n",
      "Loss: 1.8634306192398071\n",
      "Loss: 1.8518773317337036\n",
      "Loss: 1.8489251136779785\n",
      "Epoch: 3\n",
      "Loss: 1.858744502067566\n",
      "Loss: 1.8586764335632324\n",
      "Loss: 1.8488552570343018\n",
      "Loss: 1.8413487672805786\n",
      "Loss: 1.8351621627807617\n",
      "Loss: 1.8402704000473022\n",
      "Loss: 1.8285545110702515\n",
      "Loss: 1.821189284324646\n",
      "Loss: 1.8254063129425049\n",
      "Loss: 1.8269171714782715\n",
      "Loss: 1.8312968015670776\n",
      "Loss: 1.8331974744796753\n",
      "Loss: 1.8269389867782593\n",
      "Loss: 1.8179293870925903\n",
      "Loss: 1.8205652236938477\n",
      "Loss: 1.8229539394378662\n",
      "Loss: 1.812586784362793\n",
      "Loss: 1.80829656124115\n",
      "Loss: 1.8121495246887207\n",
      "Loss: 1.8101789951324463\n",
      "Loss: 1.814485788345337\n",
      "Loss: 1.8140722513198853\n",
      "Loss: 1.8079376220703125\n",
      "Loss: 1.801262617111206\n",
      "Loss: 1.8003289699554443\n",
      "Epoch: 4\n",
      "Loss: 1.7886985540390015\n",
      "Loss: 1.8052160739898682\n",
      "Loss: 1.8010858297348022\n",
      "Loss: 1.7934980392456055\n",
      "Loss: 1.7893561124801636\n",
      "Loss: 1.7884843349456787\n",
      "Loss: 1.7906593084335327\n",
      "Loss: 1.8077843189239502\n",
      "Loss: 1.7912043333053589\n",
      "Loss: 1.7932493686676025\n",
      "Loss: 1.7937792539596558\n",
      "Loss: 1.7777951955795288\n",
      "Loss: 1.7757819890975952\n",
      "Loss: 1.7912930250167847\n",
      "Loss: 1.7784128189086914\n",
      "Loss: 1.769045352935791\n",
      "Loss: 1.768351435661316\n",
      "Loss: 1.7518259286880493\n",
      "Loss: 1.7696796655654907\n",
      "Loss: 1.7710095643997192\n",
      "Loss: 1.7724604606628418\n",
      "Loss: 1.7645153999328613\n",
      "Loss: 1.7568466663360596\n",
      "Loss: 1.759461760520935\n",
      "Loss: 1.7604725360870361\n",
      "Epoch: 5\n",
      "Loss: 1.7557828426361084\n",
      "Loss: 1.748642921447754\n",
      "Loss: 1.753023386001587\n",
      "Loss: 1.7449442148208618\n",
      "Loss: 1.7522273063659668\n",
      "Loss: 1.750067114830017\n",
      "Loss: 1.7473236322402954\n",
      "Loss: 1.7444641590118408\n",
      "Loss: 1.7584784030914307\n",
      "Loss: 1.7385119199752808\n",
      "Loss: 1.7313729524612427\n",
      "Loss: 1.7329601049423218\n",
      "Loss: 1.7325884103775024\n",
      "Loss: 1.7289003133773804\n",
      "Loss: 1.7263280153274536\n",
      "Loss: 1.7267581224441528\n",
      "Loss: 1.7263859510421753\n",
      "Loss: 1.729708194732666\n",
      "Loss: 1.7320671081542969\n",
      "Loss: 1.7227051258087158\n",
      "Loss: 1.7201534509658813\n",
      "Loss: 1.7252551317214966\n",
      "Loss: 1.7089662551879883\n",
      "Loss: 1.7077128887176514\n",
      "Loss: 1.7152206897735596\n",
      "Epoch: 6\n",
      "Loss: 1.710418939590454\n",
      "Loss: 1.718056082725525\n",
      "Loss: 1.7000236511230469\n",
      "Loss: 1.7051973342895508\n",
      "Loss: 1.7031739950180054\n",
      "Loss: 1.7078813314437866\n",
      "Loss: 1.7000786066055298\n",
      "Loss: 1.701074481010437\n",
      "Loss: 1.6959794759750366\n",
      "Loss: 1.7040431499481201\n",
      "Loss: 1.6884589195251465\n",
      "Loss: 1.6986249685287476\n",
      "Loss: 1.6991528272628784\n",
      "Loss: 1.673903465270996\n",
      "Loss: 1.6810903549194336\n",
      "Loss: 1.6899815797805786\n",
      "Loss: 1.6958630084991455\n",
      "Loss: 1.6915875673294067\n",
      "Loss: 1.6852920055389404\n",
      "Loss: 1.6734507083892822\n",
      "Loss: 1.6631187200546265\n",
      "Loss: 1.6704782247543335\n",
      "Loss: 1.6665767431259155\n",
      "Loss: 1.6785216331481934\n",
      "Loss: 1.6702675819396973\n",
      "Epoch: 7\n",
      "Loss: 1.6636728048324585\n",
      "Loss: 1.6693086624145508\n",
      "Loss: 1.6681855916976929\n",
      "Loss: 1.6503137350082397\n",
      "Loss: 1.6605581045150757\n",
      "Loss: 1.6556644439697266\n",
      "Loss: 1.657470703125\n",
      "Loss: 1.6497533321380615\n",
      "Loss: 1.6575413942337036\n",
      "Loss: 1.6534150838851929\n",
      "Loss: 1.660223364830017\n",
      "Loss: 1.6530964374542236\n",
      "Loss: 1.6444889307022095\n",
      "Loss: 1.6570521593093872\n",
      "Loss: 1.6414613723754883\n",
      "Loss: 1.6480798721313477\n",
      "Loss: 1.637671947479248\n",
      "Loss: 1.6268919706344604\n",
      "Loss: 1.632612943649292\n",
      "Loss: 1.6412492990493774\n",
      "Loss: 1.6330201625823975\n",
      "Loss: 1.6468563079833984\n",
      "Loss: 1.6274596452713013\n",
      "Loss: 1.6266311407089233\n",
      "Loss: 1.6272624731063843\n",
      "Epoch: 8\n",
      "Loss: 1.6142463684082031\n",
      "Loss: 1.6204043626785278\n",
      "Loss: 1.6236153841018677\n",
      "Loss: 1.6200960874557495\n",
      "Loss: 1.6298189163208008\n",
      "Loss: 1.6123758554458618\n",
      "Loss: 1.6090176105499268\n",
      "Loss: 1.607236385345459\n",
      "Loss: 1.6164000034332275\n",
      "Loss: 1.6107181310653687\n",
      "Loss: 1.604237675666809\n",
      "Loss: 1.6033935546875\n",
      "Loss: 1.60036039352417\n",
      "Loss: 1.606475591659546\n",
      "Loss: 1.59995698928833\n",
      "Loss: 1.6060949563980103\n",
      "Loss: 1.5923185348510742\n",
      "Loss: 1.5938184261322021\n",
      "Loss: 1.5995100736618042\n",
      "Loss: 1.5812571048736572\n",
      "Loss: 1.5846405029296875\n",
      "Loss: 1.5789382457733154\n",
      "Loss: 1.5955283641815186\n",
      "Loss: 1.5807567834854126\n",
      "Loss: 1.5895373821258545\n",
      "Epoch: 9\n",
      "Loss: 1.584189772605896\n",
      "Loss: 1.5853166580200195\n",
      "Loss: 1.5859479904174805\n",
      "Loss: 1.5792661905288696\n",
      "Loss: 1.5799715518951416\n",
      "Loss: 1.5736223459243774\n",
      "Loss: 1.5597940683364868\n",
      "Loss: 1.5749495029449463\n",
      "Loss: 1.5572726726531982\n",
      "Loss: 1.564207673072815\n",
      "Loss: 1.5732219219207764\n",
      "Loss: 1.5601388216018677\n",
      "Loss: 1.5631685256958008\n",
      "Loss: 1.546006441116333\n",
      "Loss: 1.5668193101882935\n",
      "Loss: 1.550215482711792\n",
      "Loss: 1.551505446434021\n",
      "Loss: 1.55423104763031\n",
      "Loss: 1.548134684562683\n",
      "Loss: 1.543508768081665\n",
      "Loss: 1.5403791666030884\n",
      "Loss: 1.5380765199661255\n",
      "Loss: 1.5387883186340332\n",
      "Loss: 1.5490334033966064\n",
      "Loss: 1.535067081451416\n",
      "Epoch: 10\n",
      "Loss: 1.5379153490066528\n",
      "Loss: 1.5411651134490967\n",
      "Loss: 1.5249472856521606\n",
      "Loss: 1.5248298645019531\n",
      "Loss: 1.5326628684997559\n",
      "Loss: 1.5210232734680176\n",
      "Loss: 1.5291588306427002\n",
      "Loss: 1.5169026851654053\n",
      "Loss: 1.529549241065979\n",
      "Loss: 1.5210323333740234\n",
      "Loss: 1.530480146408081\n",
      "Loss: 1.5287848711013794\n",
      "Loss: 1.5239354372024536\n",
      "Loss: 1.5125447511672974\n",
      "Loss: 1.5214587450027466\n",
      "Loss: 1.5154016017913818\n",
      "Loss: 1.518242597579956\n",
      "Loss: 1.5162179470062256\n",
      "Loss: 1.506348729133606\n",
      "Loss: 1.5033024549484253\n",
      "Loss: 1.4985865354537964\n",
      "Loss: 1.5065007209777832\n",
      "Loss: 1.5005667209625244\n",
      "Loss: 1.4996552467346191\n",
      "Loss: 1.4952991008758545\n",
      "Epoch: 11\n",
      "Loss: 1.5080478191375732\n",
      "Loss: 1.4908311367034912\n",
      "Loss: 1.4994161128997803\n",
      "Loss: 1.4949897527694702\n",
      "Loss: 1.488743543624878\n",
      "Loss: 1.492874264717102\n",
      "Loss: 1.489747166633606\n",
      "Loss: 1.4896106719970703\n",
      "Loss: 1.4777827262878418\n",
      "Loss: 1.4850765466690063\n",
      "Loss: 1.4917460680007935\n",
      "Loss: 1.4799518585205078\n",
      "Loss: 1.4754905700683594\n",
      "Loss: 1.4725242853164673\n",
      "Loss: 1.4767204523086548\n",
      "Loss: 1.479814887046814\n",
      "Loss: 1.4653489589691162\n",
      "Loss: 1.4644691944122314\n",
      "Loss: 1.4628942012786865\n",
      "Loss: 1.4601719379425049\n",
      "Loss: 1.465701699256897\n",
      "Loss: 1.4593979120254517\n",
      "Loss: 1.4679663181304932\n",
      "Loss: 1.4583802223205566\n",
      "Loss: 1.4610309600830078\n",
      "Epoch: 12\n",
      "Loss: 1.448925495147705\n",
      "Loss: 1.4572515487670898\n",
      "Loss: 1.4386154413223267\n",
      "Loss: 1.4568836688995361\n",
      "Loss: 1.4384959936141968\n",
      "Loss: 1.4421230554580688\n",
      "Loss: 1.4355839490890503\n",
      "Loss: 1.4458401203155518\n",
      "Loss: 1.441884994506836\n",
      "Loss: 1.4321227073669434\n",
      "Loss: 1.434471845626831\n",
      "Loss: 1.4341881275177002\n",
      "Loss: 1.4403631687164307\n",
      "Loss: 1.4233955144882202\n",
      "Loss: 1.4341199398040771\n",
      "Loss: 1.4426263570785522\n",
      "Loss: 1.4249298572540283\n",
      "Loss: 1.433681607246399\n",
      "Loss: 1.4185515642166138\n",
      "Loss: 1.4249159097671509\n",
      "Loss: 1.4260590076446533\n",
      "Loss: 1.4113421440124512\n",
      "Loss: 1.4245623350143433\n",
      "Loss: 1.4160130023956299\n",
      "Loss: 1.4243594408035278\n",
      "Epoch: 13\n",
      "Loss: 1.4183619022369385\n",
      "Loss: 1.4074277877807617\n",
      "Loss: 1.4103423357009888\n",
      "Loss: 1.4077712297439575\n",
      "Loss: 1.4004309177398682\n",
      "Loss: 1.4103583097457886\n",
      "Loss: 1.3922282457351685\n",
      "Loss: 1.402644395828247\n",
      "Loss: 1.3851982355117798\n",
      "Loss: 1.4015436172485352\n",
      "Loss: 1.3975157737731934\n",
      "Loss: 1.3907362222671509\n",
      "Loss: 1.3883217573165894\n",
      "Loss: 1.3945331573486328\n",
      "Loss: 1.3851892948150635\n",
      "Loss: 1.3965342044830322\n",
      "Loss: 1.3891358375549316\n",
      "Loss: 1.379565954208374\n",
      "Loss: 1.389263391494751\n",
      "Loss: 1.385461688041687\n",
      "Loss: 1.3800265789031982\n",
      "Loss: 1.373332142829895\n",
      "Loss: 1.3808480501174927\n",
      "Loss: 1.3793928623199463\n",
      "Loss: 1.3782265186309814\n",
      "Epoch: 14\n",
      "Loss: 1.3775274753570557\n",
      "Loss: 1.370842456817627\n",
      "Loss: 1.3710136413574219\n",
      "Loss: 1.3668668270111084\n",
      "Loss: 1.376764178276062\n",
      "Loss: 1.3564255237579346\n",
      "Loss: 1.3595247268676758\n",
      "Loss: 1.3554118871688843\n",
      "Loss: 1.3608405590057373\n",
      "Loss: 1.359342098236084\n",
      "Loss: 1.3550043106079102\n",
      "Loss: 1.347169280052185\n",
      "Loss: 1.3574328422546387\n",
      "Loss: 1.3523744344711304\n",
      "Loss: 1.3488445281982422\n",
      "Loss: 1.3581353425979614\n",
      "Loss: 1.3502020835876465\n",
      "Loss: 1.3594272136688232\n",
      "Loss: 1.3483647108078003\n",
      "Loss: 1.3327040672302246\n",
      "Loss: 1.3427741527557373\n",
      "Loss: 1.3421260118484497\n",
      "Loss: 1.340819001197815\n",
      "Loss: 1.324979543685913\n",
      "Loss: 1.3394399881362915\n",
      "Epoch: 15\n",
      "Loss: 1.3275259733200073\n",
      "Loss: 1.3367286920547485\n",
      "Loss: 1.331984043121338\n",
      "Loss: 1.3344805240631104\n",
      "Loss: 1.3192873001098633\n",
      "Loss: 1.3264976739883423\n",
      "Loss: 1.3317123651504517\n",
      "Loss: 1.3204364776611328\n",
      "Loss: 1.3249725103378296\n",
      "Loss: 1.3181090354919434\n",
      "Loss: 1.3062652349472046\n",
      "Loss: 1.3105041980743408\n",
      "Loss: 1.3102729320526123\n",
      "Loss: 1.3073601722717285\n",
      "Loss: 1.3058125972747803\n",
      "Loss: 1.313624620437622\n",
      "Loss: 1.3088126182556152\n",
      "Loss: 1.309417486190796\n",
      "Loss: 1.3053990602493286\n",
      "Loss: 1.3024847507476807\n",
      "Loss: 1.3046456575393677\n",
      "Loss: 1.2910767793655396\n",
      "Loss: 1.2939203977584839\n",
      "Loss: 1.2947611808776855\n",
      "Loss: 1.3036680221557617\n",
      "Epoch: 16\n",
      "Loss: 1.298046588897705\n",
      "Loss: 1.2831634283065796\n",
      "Loss: 1.2923897504806519\n",
      "Loss: 1.2895656824111938\n",
      "Loss: 1.2864090204238892\n",
      "Loss: 1.2810325622558594\n",
      "Loss: 1.274129033088684\n",
      "Loss: 1.2788496017456055\n",
      "Loss: 1.2800257205963135\n",
      "Loss: 1.2792441844940186\n",
      "Loss: 1.2821786403656006\n",
      "Loss: 1.268831491470337\n",
      "Loss: 1.2674212455749512\n",
      "Loss: 1.2746574878692627\n",
      "Loss: 1.2638920545578003\n",
      "Loss: 1.2689779996871948\n",
      "Loss: 1.2716526985168457\n",
      "Loss: 1.2687039375305176\n",
      "Loss: 1.2660462856292725\n",
      "Loss: 1.2627605199813843\n",
      "Loss: 1.2680485248565674\n",
      "Loss: 1.2549465894699097\n",
      "Loss: 1.2582728862762451\n",
      "Loss: 1.2616279125213623\n",
      "Loss: 1.25136399269104\n",
      "Epoch: 17\n",
      "Loss: 1.261560082435608\n",
      "Loss: 1.2540568113327026\n",
      "Loss: 1.249359130859375\n",
      "Loss: 1.257698893547058\n",
      "Loss: 1.2427270412445068\n",
      "Loss: 1.246473789215088\n",
      "Loss: 1.2495760917663574\n",
      "Loss: 1.2470481395721436\n",
      "Loss: 1.2489612102508545\n",
      "Loss: 1.2384134531021118\n",
      "Loss: 1.2375462055206299\n",
      "Loss: 1.239692211151123\n",
      "Loss: 1.238275170326233\n",
      "Loss: 1.2265772819519043\n",
      "Loss: 1.2493771314620972\n",
      "Loss: 1.23417329788208\n",
      "Loss: 1.2227436304092407\n",
      "Loss: 1.2358053922653198\n",
      "Loss: 1.2297443151474\n",
      "Loss: 1.2186524868011475\n",
      "Loss: 1.2232370376586914\n",
      "Loss: 1.2204387187957764\n",
      "Loss: 1.2171316146850586\n",
      "Loss: 1.2178070545196533\n",
      "Loss: 1.2166451215744019\n",
      "Epoch: 18\n",
      "Loss: 1.2192810773849487\n",
      "Loss: 1.211821436882019\n",
      "Loss: 1.214249849319458\n",
      "Loss: 1.2130087614059448\n",
      "Loss: 1.2135192155838013\n",
      "Loss: 1.2061551809310913\n",
      "Loss: 1.1998430490493774\n",
      "Loss: 1.1993364095687866\n",
      "Loss: 1.1994764804840088\n",
      "Loss: 1.1981161832809448\n",
      "Loss: 1.2060736417770386\n",
      "Loss: 1.2014939785003662\n",
      "Loss: 1.2046818733215332\n",
      "Loss: 1.1945390701293945\n",
      "Loss: 1.1924532651901245\n",
      "Loss: 1.1925160884857178\n",
      "Loss: 1.182218074798584\n",
      "Loss: 1.2117409706115723\n",
      "Loss: 1.195162296295166\n",
      "Loss: 1.1873736381530762\n",
      "Loss: 1.1888322830200195\n",
      "Loss: 1.1853625774383545\n",
      "Loss: 1.190861463546753\n",
      "Loss: 1.1909449100494385\n",
      "Loss: 1.1753976345062256\n",
      "Epoch: 19\n",
      "Loss: 1.1835626363754272\n",
      "Predictions: ['Title:Bar Chart 7, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 43, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1781697273254395\n",
      "Predictions: ['Title:Bar Chart 5, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 37, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1735142469406128\n",
      "Predictions: ['Title:Bar Chart 21, x-axis: Category, y-axis: Values', 'Title:Bar Chart 44, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.17739737033844\n",
      "Predictions: ['Title:Bar Chart 11, x-axis: Category, y-axis: Values', 'Title:Bar Chart 16, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1919796466827393\n",
      "Predictions: ['Title:Bar Chart 2, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 20, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1658039093017578\n",
      "Predictions: ['Title:Bar Chart 31, x-axis: Category, y-axis: Values', 'Title:Bar Chart 19, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1696453094482422\n",
      "Predictions: ['Title:Bar Chart 27, x-axis: Category, y-axis: Values', 'Title:Bar Chart 18, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1675136089324951\n",
      "Predictions: ['Title:Bar Chart 34, x-axis: Category, y-axis: Values', 'Title:Bar Chart 47, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1671984195709229\n",
      "Predictions: ['Title:Bar Chart 41, x-axis: Category, y-axis: Values', 'Title:Bar Chart 38, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1670753955841064\n",
      "Predictions: ['Title:Bar Chart 6, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 46, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1612483263015747\n",
      "Predictions: ['Title:Bar Chart 3, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 50, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1643092632293701\n",
      "Predictions: ['Title:Bar Chart 22, x-axis: Category, y-axis: Values', 'Title:Bar Chart 8, x-axis: Category, y-axis: Values A']\n",
      "Loss: 1.1620280742645264\n",
      "Predictions: ['Title:Bar Chart 29, x-axis: Category, y-axis: Values', 'Title:Bar Chart 17, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1599396467208862\n",
      "Predictions: ['Title:Bar Chart 1, x-axis: Category, y-axis: Values A', 'Title:Bar Chart 26, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1735154390335083\n",
      "Predictions: ['Title:Bar Chart 10, x-axis: Category, y-axis: Values', 'Title:Bar Chart 49, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1568938493728638\n",
      "Predictions: ['Title:Bar Chart 24, x-axis: Category, y-axis: Values', 'Title:Bar Chart 32, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1600208282470703\n",
      "Predictions: ['Title:Bar Chart 28, x-axis: Category, y-axis: Values', 'Title:Bar Chart 30, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.158132791519165\n",
      "Predictions: ['Title:Bar Chart 14, x-axis: Category, y-axis: Values', 'Title:Bar Chart 13, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.158908724784851\n",
      "Predictions: ['Title:Bar Chart 15, x-axis: Category, y-axis: Values', 'Title:Bar Chart 12, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1550577878952026\n",
      "Predictions: ['Title:Bar Chart 42, x-axis: Category, y-axis: Values', 'Title:Bar Chart 9, x-axis: Category, y-axis: Values A']\n",
      "Loss: 1.1438597440719604\n",
      "Predictions: ['Title:Bar Chart 23, x-axis: Category, y-axis: Values', 'Title:Bar Chart 40, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1508060693740845\n",
      "Predictions: ['Title:Bar Chart 33, x-axis: Category, y-axis: Values', 'Title:Bar Chart 36, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1431193351745605\n",
      "Predictions: ['Title:Bar Chart 35, x-axis: Category, y-axis: Values', 'Title:Bar Chart 4, x-axis: Category, y-axis: Values A']\n",
      "Loss: 1.1442750692367554\n",
      "Predictions: ['Title:Bar Chart 48, x-axis: Category, y-axis: Values', 'Title:Bar Chart 45, x-axis: Category, y-axis: Values']\n",
      "Loss: 1.1521049737930298\n",
      "Predictions: ['Title:Bar Chart 25, x-axis: Category, y-axis: Values', 'Title:Bar Chart 39, x-axis: Category, y-axis: Values']\n",
      "Epoch: 20\n",
      "Loss: 1.1432806253433228\n",
      "Loss: 1.1371909379959106\n",
      "Loss: 1.1365751028060913\n",
      "Loss: 1.140196442604065\n",
      "Loss: 1.1360278129577637\n",
      "Loss: 1.1273033618927002\n",
      "Loss: 1.1402757167816162\n",
      "Loss: 1.124610185623169\n",
      "Loss: 1.1290552616119385\n",
      "Loss: 1.1325409412384033\n",
      "Loss: 1.1288034915924072\n",
      "Loss: 1.1252104043960571\n",
      "Loss: 1.1239798069000244\n",
      "Loss: 1.1262818574905396\n",
      "Loss: 1.1224584579467773\n",
      "Loss: 1.1273926496505737\n",
      "Loss: 1.1230536699295044\n",
      "Loss: 1.1216368675231934\n",
      "Loss: 1.1194753646850586\n",
      "Loss: 1.1208223104476929\n",
      "Loss: 1.1176947355270386\n",
      "Loss: 1.1130003929138184\n",
      "Loss: 1.116119384765625\n",
      "Loss: 1.103113055229187\n",
      "Loss: 1.107638955116272\n",
      "Epoch: 21\n",
      "Loss: 1.100551962852478\n",
      "Loss: 1.1016992330551147\n",
      "Loss: 1.10788893699646\n",
      "Loss: 1.0915040969848633\n",
      "Loss: 1.1005306243896484\n",
      "Loss: 1.0988155603408813\n",
      "Loss: 1.1032748222351074\n",
      "Loss: 1.0905497074127197\n",
      "Loss: 1.0984976291656494\n",
      "Loss: 1.1007475852966309\n",
      "Loss: 1.091232419013977\n",
      "Loss: 1.088038682937622\n",
      "Loss: 1.0920517444610596\n",
      "Loss: 1.0876585245132446\n",
      "Loss: 1.094004511833191\n",
      "Loss: 1.0788633823394775\n",
      "Loss: 1.08334481716156\n",
      "Loss: 1.080881118774414\n",
      "Loss: 1.075753927230835\n",
      "Loss: 1.0862269401550293\n",
      "Loss: 1.0797401666641235\n",
      "Loss: 1.0835803747177124\n",
      "Loss: 1.0723730325698853\n",
      "Loss: 1.0795001983642578\n",
      "Loss: 1.080726146697998\n",
      "Epoch: 22\n",
      "Loss: 1.069634199142456\n",
      "Loss: 1.0750178098678589\n",
      "Loss: 1.0702399015426636\n",
      "Loss: 1.0639755725860596\n",
      "Loss: 1.0679500102996826\n",
      "Loss: 1.0561455488204956\n",
      "Loss: 1.065589189529419\n",
      "Loss: 1.0606193542480469\n",
      "Loss: 1.0616117715835571\n",
      "Loss: 1.068464756011963\n",
      "Loss: 1.049049735069275\n",
      "Loss: 1.0584439039230347\n",
      "Loss: 1.0595314502716064\n",
      "Loss: 1.0532400608062744\n",
      "Loss: 1.0474005937576294\n",
      "Loss: 1.0452768802642822\n",
      "Loss: 1.0445091724395752\n",
      "Loss: 1.0585224628448486\n",
      "Loss: 1.0417373180389404\n",
      "Loss: 1.0494608879089355\n",
      "Loss: 1.0437065362930298\n",
      "Loss: 1.0487732887268066\n",
      "Loss: 1.0449612140655518\n",
      "Loss: 1.0369386672973633\n",
      "Loss: 1.0558154582977295\n",
      "Epoch: 23\n",
      "Loss: 1.04639732837677\n",
      "Loss: 1.0296837091445923\n",
      "Loss: 1.0298800468444824\n",
      "Loss: 1.0241340398788452\n",
      "Loss: 1.0338348150253296\n",
      "Loss: 1.034051775932312\n",
      "Loss: 1.031530499458313\n",
      "Loss: 1.031042456626892\n",
      "Loss: 1.0241854190826416\n",
      "Loss: 1.0203680992126465\n",
      "Loss: 1.0202891826629639\n",
      "Loss: 1.0298596620559692\n",
      "Loss: 1.0181362628936768\n",
      "Loss: 1.019180178642273\n",
      "Loss: 1.0111286640167236\n",
      "Loss: 1.0182667970657349\n",
      "Loss: 1.0137296915054321\n",
      "Loss: 1.0111051797866821\n",
      "Loss: 1.0042113065719604\n",
      "Loss: 1.0126937627792358\n",
      "Loss: 1.00779128074646\n",
      "Loss: 1.0028860569000244\n",
      "Loss: 1.0034178495407104\n",
      "Loss: 1.0046772956848145\n",
      "Loss: 1.007394790649414\n",
      "Epoch: 24\n",
      "Loss: 1.00089693069458\n",
      "Loss: 0.9946145415306091\n",
      "Loss: 1.008137583732605\n",
      "Loss: 1.0033408403396606\n",
      "Loss: 0.9934031963348389\n",
      "Loss: 0.9945238828659058\n",
      "Loss: 1.0032745599746704\n",
      "Loss: 0.991022527217865\n",
      "Loss: 0.997453510761261\n",
      "Loss: 0.9850925803184509\n",
      "Loss: 0.9921122193336487\n",
      "Loss: 0.9862369894981384\n",
      "Loss: 0.9824467301368713\n",
      "Loss: 0.9889949560165405\n",
      "Loss: 0.9760876893997192\n",
      "Loss: 0.9826282858848572\n",
      "Loss: 0.9760937094688416\n",
      "Loss: 0.9754050970077515\n",
      "Loss: 0.9810153245925903\n",
      "Loss: 0.9691439867019653\n",
      "Loss: 0.9761627912521362\n",
      "Loss: 0.9847480058670044\n",
      "Loss: 0.9709922075271606\n",
      "Loss: 0.9705138206481934\n",
      "Loss: 0.9796249270439148\n",
      "Epoch: 25\n",
      "Loss: 0.9660478234291077\n",
      "Loss: 0.9630575776100159\n",
      "Loss: 0.9641930460929871\n",
      "Loss: 0.9673168063163757\n",
      "Loss: 0.9585477113723755\n",
      "Loss: 0.966102123260498\n",
      "Loss: 0.9545328617095947\n",
      "Loss: 0.9649935364723206\n",
      "Loss: 0.9597644209861755\n",
      "Loss: 0.9550053477287292\n",
      "Loss: 0.9518768787384033\n",
      "Loss: 0.956790030002594\n",
      "Loss: 0.9648962616920471\n",
      "Loss: 0.9484061002731323\n",
      "Loss: 0.9490463733673096\n",
      "Loss: 0.9542269110679626\n",
      "Loss: 0.9544060230255127\n",
      "Loss: 0.9480592012405396\n",
      "Loss: 0.9467516541481018\n",
      "Loss: 0.9380415678024292\n",
      "Loss: 0.9528236985206604\n",
      "Loss: 0.9464306235313416\n",
      "Loss: 0.9401832818984985\n",
      "Loss: 0.935704231262207\n",
      "Loss: 0.9407791495323181\n",
      "Epoch: 26\n",
      "Loss: 0.9382638335227966\n",
      "Loss: 0.9356868267059326\n",
      "Loss: 0.9362512230873108\n",
      "Loss: 0.9369648694992065\n",
      "Loss: 0.9329073429107666\n",
      "Loss: 0.9448166489601135\n",
      "Loss: 0.9363769292831421\n",
      "Loss: 0.932755708694458\n",
      "Loss: 0.9268801808357239\n",
      "Loss: 0.9256981611251831\n",
      "Loss: 0.921398937702179\n",
      "Loss: 0.9251241683959961\n",
      "Loss: 0.9245495200157166\n",
      "Loss: 0.9148004651069641\n",
      "Loss: 0.9163640141487122\n",
      "Loss: 0.9212531447410583\n",
      "Loss: 0.9162884950637817\n",
      "Loss: 0.9157913327217102\n",
      "Loss: 0.9251691102981567\n",
      "Loss: 0.9207943677902222\n",
      "Loss: 0.909065842628479\n",
      "Loss: 0.9152337312698364\n",
      "Loss: 0.9047344326972961\n",
      "Loss: 0.9049779176712036\n",
      "Loss: 0.9027347564697266\n",
      "Epoch: 27\n",
      "Loss: 0.9082217812538147\n",
      "Loss: 0.9090882539749146\n",
      "Loss: 0.902827262878418\n",
      "Loss: 0.8971927165985107\n",
      "Loss: 0.899556577205658\n",
      "Loss: 0.8949089646339417\n",
      "Loss: 0.9041159749031067\n",
      "Loss: 0.8980190753936768\n",
      "Loss: 0.8956262469291687\n",
      "Loss: 0.898116946220398\n",
      "Loss: 0.8991427421569824\n",
      "Loss: 0.8942915797233582\n",
      "Loss: 0.8889205455780029\n",
      "Loss: 0.8963415622711182\n",
      "Loss: 0.8932974934577942\n",
      "Loss: 0.8855058550834656\n",
      "Loss: 0.8860144019126892\n",
      "Loss: 0.8872883319854736\n",
      "Loss: 0.8866185545921326\n",
      "Loss: 0.8808947205543518\n",
      "Loss: 0.8884265422821045\n",
      "Loss: 0.8832061290740967\n",
      "Loss: 0.8752045035362244\n",
      "Loss: 0.8771626949310303\n",
      "Loss: 0.8773999214172363\n",
      "Epoch: 28\n",
      "Loss: 0.8721223473548889\n",
      "Loss: 0.8748834729194641\n",
      "Loss: 0.8751128911972046\n",
      "Loss: 0.8753591179847717\n",
      "Loss: 0.8764852285385132\n",
      "Loss: 0.8652740716934204\n",
      "Loss: 0.875022292137146\n",
      "Loss: 0.8635148406028748\n",
      "Loss: 0.8620193004608154\n",
      "Loss: 0.8648316860198975\n",
      "Loss: 0.8672059178352356\n",
      "Loss: 0.8585109114646912\n",
      "Loss: 0.8607147932052612\n",
      "Loss: 0.8593941926956177\n",
      "Loss: 0.8593662977218628\n",
      "Loss: 0.8576396703720093\n",
      "Loss: 0.8597940802574158\n",
      "Loss: 0.856037437915802\n",
      "Loss: 0.8532622456550598\n",
      "Loss: 0.8572234511375427\n",
      "Loss: 0.8535043001174927\n",
      "Loss: 0.8503044843673706\n",
      "Loss: 0.8575993180274963\n",
      "Loss: 0.8466644287109375\n",
      "Loss: 0.8443282842636108\n",
      "Epoch: 29\n",
      "Loss: 0.8431362509727478\n",
      "Loss: 0.8497045636177063\n",
      "Loss: 0.8461095690727234\n",
      "Loss: 0.8428102731704712\n",
      "Loss: 0.8438883423805237\n",
      "Loss: 0.8419403433799744\n",
      "Loss: 0.8394681215286255\n",
      "Loss: 0.8428813815116882\n",
      "Loss: 0.8328554034233093\n",
      "Loss: 0.8314420580863953\n",
      "Loss: 0.8394563794136047\n",
      "Loss: 0.8361161351203918\n",
      "Loss: 0.832716166973114\n",
      "Loss: 0.8283214569091797\n",
      "Loss: 0.8269911408424377\n",
      "Loss: 0.8255409002304077\n",
      "Loss: 0.8266273736953735\n",
      "Loss: 0.8322468996047974\n",
      "Loss: 0.827510416507721\n",
      "Loss: 0.8267640471458435\n",
      "Loss: 0.8266282081604004\n",
      "Loss: 0.8205902576446533\n",
      "Loss: 0.8218914866447449\n",
      "Loss: 0.8227841854095459\n",
      "Loss: 0.8173394203186035\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# EPOCHS = 30\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#   print(\"Epoch:\", epoch)\n",
    "#   # processor.image_processor.is_vqa = False\n",
    "#   for idx, batch in enumerate(train_dataloader):\n",
    "#     labels = batch.pop(\"labels\").to(device)\n",
    "#     flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "#     attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "#     outputs = model(flattened_patches=flattened_patches,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     labels=labels)\n",
    "\n",
    "#     loss = outputs.loss\n",
    "\n",
    "#     print(\"Loss:\", loss.item())\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     if (epoch + 1) % 20 == 0:\n",
    "#         model.eval()\n",
    "\n",
    "#         predictions = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask)\n",
    "#         print(\"Predictions:\", processor.batch_decode(predictions, skip_special_tokens=True))\n",
    "\n",
    "#         model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb0987-e662-460b-8663-514cfd1c7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: Model_Weights/BarChart_DePlot.pth\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# # 1. Create models directory\n",
    "# MODEL_PATH = Path(\"Model_Weights\")\n",
    "# MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # 2. Create model save path\n",
    "# MODEL_NAME = \"BarChart_DePlot.pth\"\n",
    "# MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# # 3. Save the model state dict\n",
    "# print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "# torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "#            f=MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f6ecd-1adc-425a-8c69-227846d1c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c64c6-f095-4bd7-bed4-d137c401805d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pix2StructForConditionalGeneration(\n",
       "  (encoder): Pix2StructVisionModel(\n",
       "    (embeddings): Pix2StructVisionEmbeddings(\n",
       "      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (row_embedder): Embedding(4096, 768)\n",
       "      (column_embedder): Embedding(4096, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Pix2StructVisionEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (1): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (2): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (3): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (4): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (5): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (6): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (7): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (8): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (9): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (10): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "        (11): Pix2StructVisionLayer(\n",
       "          (attention): Pix2StructVisionAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (mlp): Pix2StructVisionMlp(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
       "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Pix2StructLayerNorm()\n",
       "  )\n",
       "  (decoder): Pix2StructTextModel(\n",
       "    (embed_tokens): Embedding(50244, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 12)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Pix2StructTextBlock(\n",
       "        (self_attention): Pix2StructTextLayerSelfAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n",
       "          (attention): Pix2StructTextAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): Pix2StructTextLayerFF(\n",
       "          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): Pix2StructLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pix2StructLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (lm_head): Linear(in_features=768, out_features=50244, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd620c-821f-40b3-a420-0f74d177d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f=\"Model_Weights/BarChart_DePlot.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fcc7b-ca7d-440c-ae04-60a4a51dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d9ef9-394e-4565-a6cb-86a74773a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6111b-a460-4308-b24d-e80deb0c1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfc33f-cdf9-42af-8b1b-bd81408ff600",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c440fa-760c-4382-9921-2e76bbc49e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(input_image):\n",
    "    # Load the uploaded image\n",
    "    image = Image.open(input_image)\n",
    "    inputs = processor(images=image, text=\"Chart Data\", return_tensors=\"pt\").to(device)\n",
    "    predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "    return processor.decode(predictions[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc7a8e-9abe-4c97-b0ae-f7f8e31aa555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_analyser(image_path):\n",
    "  data=process_image(image_path)\n",
    "  #print(data)\n",
    "  prompt_template = \"\"\"\n",
    "  You are given data for a BarChart delimited by triple backticks :\n",
    "  ```{data}```\n",
    "  The output must be :\n",
    "  -Returned in a Table format\n",
    "  -Table must have only 2 appropriate columns\n",
    "  \"\"\"\n",
    "\n",
    "  llm = OpenAI(temperature=0, max_tokens=1024)\n",
    "  llm_chain = LLMChain(\n",
    "      llm=llm,\n",
    "      prompt=PromptTemplate(template=prompt_template, input_variables=[\"data\"])\n",
    "  )\n",
    "  answer = llm_chain({\"data\":data})\n",
    "  return answer[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f54546-43b3-4a58-a061-fc64ab51de72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import HTML\n",
    "# HTML(image_analyser(\"Data_Barcharts/chart_1.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203220c-8150-4922-8612-15c82628e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_querying(query,data):\n",
    "  if not query or query ==\"\":\n",
    "   return \"Please enter you Question \"\n",
    "  prompt_template = \"\"\"\n",
    "  You are given a table of data for a BarChart delimited by triple backticks :\n",
    "  ```{data}```\n",
    "  From the table answer the following query:\n",
    "  query:{query}\n",
    "  answer:\"\"\"\n",
    "\n",
    "  llm = OpenAI(temperature=0, max_tokens=1024)\n",
    "  llm_chain = LLMChain(\n",
    "      llm=llm,\n",
    "      prompt=PromptTemplate(template=prompt_template, input_variables=[\"query\",\"data\"])\n",
    "  )\n",
    "  answer = llm_chain({\"query\":query,\"data\":data})\n",
    "  return answer[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b4b70-c17b-49ef-94be-2cac833e76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "  with gr.Row():\n",
    "    with gr.Column():\n",
    "      image=gr.Image(type='filepath')\n",
    "      # table=gr.Text(label=\"Show Table\")\n",
    "      # btn1=gr.Button(\"Display Table for the Chart\")\n",
    "      # btn1.click(image_analyser,inputs=[image],outputs=[table])\n",
    "      # #btn=gr.Button(\"Submit\")\n",
    "    with gr.Column():\n",
    "      table=gr.Text(label=\"Show Table\")\n",
    "      btn1=gr.Button(\"Display Table for the Chart\")\n",
    "      btn1.click(image_analyser,inputs=[image],outputs=[table])\n",
    "      query=gr.Text(label=\"Ask your Question\")\n",
    "      btn=gr.Button(\"Submit\")\n",
    "      answer=gr.Text(label=\"Output\")\n",
    "      btn.click(image_querying, inputs=[query,table], outputs=[answer])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33360e83-1689-43b0-8208-a24ed6b47bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://bb9cca54c28ca2d75a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bb9cca54c28ca2d75a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74122d6-0ad4-4e47-af12-ea9f9bfeff2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
